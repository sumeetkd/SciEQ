{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " https://stackoverflow.com/questions/42068474/tfidfvectorizer-how-does-the-vectorizer-with-fixed-vocab-deal-with-new-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2orc_pdf = '/home/sumeetkd/codes/Quazar/purchaser/S2ORC/s2orc/data/pdf_parses/sample.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/203 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values are presented as number only or median (range). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import io, json, re\n",
    "from tqdm import tqdm\n",
    "pattern = re.compile('Agilent')\n",
    "with open(s2orc_pdf, 'r') as f_out:\n",
    "    for line in tqdm(f_out.readlines()):\n",
    "        pdf_parse = json.loads(line)\n",
    "        sections = pdf_parse['body_text']\n",
    "        for section in sections:\n",
    "            print(section['text'])\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:00<00:00, 1897.11it/s]\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('the')\n",
    "corpus = []\n",
    "with open(s2orc_pdf, 'rb') as myfile:\n",
    "    head = [next(myfile) for x in range(160)]\n",
    "    for line in tqdm(head):\n",
    "        pdf_parse = json.loads(line)\n",
    "        #print(pdf_parse['paper_id'],'\\n')\n",
    "        sections = pdf_parse['body_text']\n",
    "        #print(section.keys())123\n",
    "        for section in sections:\n",
    "            if pattern.findall(section['text']):\n",
    "                #print(section['section'])\n",
    "                corpus.append(section['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1833"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "203it [00:00, 1296.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77499681\n",
      "94550656\n",
      "94551239\n",
      "94551546\n",
      "94552339\n",
      "94553452\n",
      "94555004\n",
      "94556881\n",
      "94559152\n",
      "159350887\n",
      "159352431\n",
      "159355456\n",
      "159355939\n",
      "159358374\n",
      "159359827\n",
      "18980190\n",
      "18980380\n",
      "18980463\n",
      "18980811\n",
      "18981111\n",
      "18981201\n",
      "18981319\n",
      "18981336\n",
      "18981358\n",
      "18981625\n",
      "18982114\n",
      "18982127\n",
      "18982341\n",
      "18982460\n",
      "18982496\n",
      "18982504\n",
      "18982512\n",
      "18982781\n",
      "18983082\n",
      "18983391\n",
      "18984129\n",
      "18984386\n",
      "18984455\n",
      "18984609\n",
      "18984674\n",
      "18984807\n",
      "18985171\n",
      "18985891\n",
      "18986705\n",
      "18986901\n",
      "18987532\n",
      "18987535\n",
      "18987990\n",
      "18988046\n",
      "18988104\n",
      "18988615\n",
      "18989067\n",
      "18989187\n",
      "18989350\n",
      "18989559\n",
      "18989713\n",
      "18989842\n",
      "199660318\n",
      "199661662\n",
      "199661683\n",
      "199661892\n",
      "199661969\n",
      "199662147\n",
      "199662295\n",
      "199662866\n",
      "199664249\n",
      "199664457\n",
      "199664595\n",
      "199664911\n",
      "199664931\n",
      "199665186\n",
      "199665266\n",
      "199665542\n",
      "199665735\n",
      "199665950\n",
      "199666740\n",
      "199666746\n",
      "199666823\n",
      "199666980\n",
      "199667935\n",
      "199667938\n",
      "199668067\n",
      "199668072\n",
      "199668440\n",
      "199668529\n",
      "199668547\n",
      "199668877\n",
      "199668887\n",
      "199668943\n",
      "199669259\n",
      "199669503\n",
      "2870029\n",
      "2870075\n",
      "2870145\n",
      "2870252\n",
      "2870542\n",
      "2870829\n",
      "2870859\n",
      "2871179\n",
      "2871295\n",
      "2871533\n",
      "2871546\n",
      "2871630\n",
      "2871678\n",
      "2871844\n",
      "2871896\n",
      "2871940\n",
      "2872418\n",
      "2872518\n",
      "2872538\n",
      "2872580\n",
      "2872613\n",
      "2872653\n",
      "2872803\n",
      "2872933\n",
      "2873021\n",
      "2873489\n",
      "2873661\n",
      "2874018\n",
      "2874048\n",
      "2874113\n",
      "2874360\n",
      "2874456\n",
      "2874531\n",
      "2874570\n",
      "2874910\n",
      "2875387\n",
      "2875502\n",
      "2876847\n",
      "2877021\n",
      "2877038\n",
      "2877272\n",
      "2877363\n",
      "2877433\n",
      "2877504\n",
      "2877521\n",
      "2878408\n",
      "2878434\n",
      "2878772\n",
      "2878780\n",
      "2878811\n",
      "2879218\n",
      "2879242\n",
      "2879409\n",
      "2879548\n",
      "2879621\n",
      "2879698\n",
      "2879848\n",
      "213463721\n",
      "213466066\n",
      "213467900\n",
      "177045333\n",
      "213341263\n",
      "213343016\n",
      "213344988\n",
      "147010188\n",
      "147010337\n",
      "147012332\n",
      "147014638\n",
      "147015306\n",
      "147016855\n",
      "116981165\n",
      "116981722\n",
      "116982275\n",
      "116984330\n",
      "116984815\n",
      "116985999\n",
      "116986096\n",
      "116987163\n",
      "116988830\n",
      "116988896\n",
      "116989707\n",
      "164944031\n",
      "164945171\n",
      "164945677\n",
      "164948735\n",
      "118860172\n",
      "118860362\n",
      "118860835\n",
      "118861869\n",
      "118862329\n",
      "118863510\n",
      "118863857\n",
      "118864086\n",
      "118864185\n",
      "118864492\n",
      "118864667\n",
      "118866026\n",
      "118867819\n",
      "118869358\n",
      "84882469\n",
      "84883393\n",
      "84886121\n",
      "84887049\n",
      "84887251\n",
      "84889665\n",
      "123621459\n",
      "123621513\n",
      "123621809\n",
      "123624411\n",
      "123624889\n",
      "123625503\n",
      "123625563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s2orc_pdf = '/home/sumeetkd/codes/Quazar/purchaser/S2ORC/s2orc/data/pdf_parses/sample.jsonl'\n",
    "with open(s2orc_pdf, 'rb') as myfile:\n",
    "    for line in tqdm(myfile):\n",
    "        metadata = json.loads(line)\n",
    "        paper_id = metadata['paper_id']\n",
    "        print(paper_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading subject abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final reading of gzip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip,io,tqdm,json\n",
    "abstracts = []\n",
    "paper_ids = []\n",
    "with gzip.open('/home/sumeetkd/codes/Quazar/purchaser/S2ORC/sample/20200705v1/full/metadata/metadata_0.jsonl.gz','rb') as gz:\n",
    "    f = io.BufferedReader(gz)\n",
    "    for line in f:\n",
    "        metadata_dict = json.loads(line)\n",
    "        paper_id = metadata_dict['paper_id']\n",
    "        mag_field_of_study = metadata_dict['mag_field_of_study']\n",
    "        abstract = metadata_dict['abstract']\n",
    "        # [TODO] add pdf abstract reader in case absent\n",
    "        if mag_field_of_study and 'Physics' in mag_field_of_study and abstract != None:\n",
    "            paper_ids.append(paper_id)\n",
    "            abstracts.append(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457344"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53364"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53364"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abstracts for a particular field coming out from a shard amount to less than 1 MB. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from second gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip,io,tqdm,json\n",
    "abstracts_1 = []\n",
    "with gzip.open('/mnt/DataSource/20200705v1/full/metadata/metadata_1.jsonl.gz','rb') as gz:\n",
    "    f = io.BufferedReader(gz)\n",
    "    for line in f:\n",
    "        metadata_dict = json.loads(line)\n",
    "        paper_id = metadata_dict['paper_id']\n",
    "        mag_field_of_study = metadata_dict['mag_field_of_study']\n",
    "        abstract = metadata_dict['abstract']\n",
    "        # [TODO] add pdf abstract reader in case absent\n",
    "        if mag_field_of_study and 'Physics' in mag_field_of_study and abstract != None:\n",
    "            paper_ids.append(paper_id)\n",
    "            abstracts_1.append(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53423"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstracts_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457344"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(abstracts_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip,io,tqdm,json\n",
    "abstracts = []\n",
    "paper_ids = []\n",
    "with gzip.open('/home/sumeetkd/codes/Quazar/purchaser/S2ORC/sample/20200705v1/full/metadata/metadata_0.jsonl.gz','rb') as gz:\n",
    "    f = io.BufferedReader(gz)\n",
    "    for line in f:\n",
    "        metadata_dict = json.loads(line)\n",
    "        paper_id = metadata_dict['paper_id']\n",
    "        mag_field_of_study = metadata_dict['mag_field_of_study']\n",
    "        abstract = metadata_dict['abstract']\n",
    "        # [TODO] add pdf abstract reader in case absent\n",
    "        if mag_field_of_study and 'Physics' in mag_field_of_study and abstract != None:\n",
    "            paper_ids.append(paper_id)\n",
    "            abstracts.append(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rules for choosing tokenization\n",
    "Drop stop words and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(document):\n",
    "    tokens = nlp(document)\n",
    "    tokens = [token.lemma_ for token in tokens if (\n",
    "        token.is_stop == False and \\\n",
    "        token.is_punct == False and \\\n",
    "        token.lemma_.strip()!= '')]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gene therapy holds potential for treating many severe diseases, such as cancer and genetic diseases. 1 Successful gene therapy depends on highefficiency gene delivery processes, in which the gene carriers have an essential role. The application of traditional viral vectors has been a challenge because of their toxicity, immunogenicity and low capability for scaling up. 2 There has long been a scientific demand for developing non-viral gene delivery systems that can overcome the drawbacks of viral vectors. 3 Non-viral gene delivery has been advanced by the rapid development of materials science and technology. Numerous novel gene delivery systems have been proposed based on functional cationic polymers, such as polyethylenimine (PEI), [4] [5] [6] [7] [8] [9] poly(2-(dimethylamino) ethyl methacrylate), 4,10 poly(L-lysine), 11 poly(aspartic acid) 12, 13 and polyamidoamine. 14 However, these non-viral gene carriers still have shortcomings, including cytotoxicity, low transfection efficiency and lack of multifunction.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plasmid',\n",
       " 'prl',\n",
       " 'CMV',\n",
       " 'reporter',\n",
       " 'gene',\n",
       " 'utilize',\n",
       " 'estimate',\n",
       " 'vitro',\n",
       " 'gene',\n",
       " 'transfection',\n",
       " 'PGMA',\n",
       " 'base',\n",
       " 'polycation',\n",
       " 'pdna',\n",
       " 'polyplexe',\n",
       " 'C6',\n",
       " 'Hep',\n",
       " 'G2',\n",
       " 'cell',\n",
       " 'line',\n",
       " 'essentially',\n",
       " 'C6',\n",
       " 'Hep',\n",
       " 'G2',\n",
       " 'cell',\n",
       " 'culture',\n",
       " '24well',\n",
       " 'plate',\n",
       " 'density',\n",
       " '6',\n",
       " '×',\n",
       " '10',\n",
       " '4',\n",
       " 'cell',\n",
       " 'DMEM',\n",
       " 'medium',\n",
       " '10',\n",
       " 'fetal',\n",
       " 'bovine',\n",
       " 'serum',\n",
       " 'solution',\n",
       " 'polycation',\n",
       " 'pdna',\n",
       " 'complex',\n",
       " 'different',\n",
       " 'n',\n",
       " 'p',\n",
       " 'ratio',\n",
       " '20',\n",
       " 'μl',\n",
       " '1.0',\n",
       " 'μg',\n",
       " 'pdna',\n",
       " 'add',\n",
       " 'transfection',\n",
       " 'medium',\n",
       " 'detailed',\n",
       " 'transfection',\n",
       " 'procedure',\n",
       " 'describe',\n",
       " 'early',\n",
       " 'work',\n",
       " '15',\n",
       " '16',\n",
       " '32',\n",
       " '33',\n",
       " 'commercial',\n",
       " 'promega',\n",
       " 'kit',\n",
       " 'luminometer',\n",
       " 'Berthold',\n",
       " 'Lumat',\n",
       " 'LB',\n",
       " '9507',\n",
       " 'Berthold',\n",
       " 'Technologies',\n",
       " 'GmbH',\n",
       " 'KG',\n",
       " 'Bad',\n",
       " 'Wildbad',\n",
       " 'Germany',\n",
       " 'quantify',\n",
       " 'luciferase',\n",
       " 'gene',\n",
       " 'expression',\n",
       " 'gene',\n",
       " 'expression',\n",
       " 'result',\n",
       " 'express',\n",
       " 'relative',\n",
       " 'light',\n",
       " 'unit',\n",
       " 'milligram',\n",
       " 'cell',\n",
       " 'protein',\n",
       " 'lysate',\n",
       " 'relative',\n",
       " 'light',\n",
       " 'unit',\n",
       " 'mg',\n",
       " 'protein',\n",
       " 'plasmid',\n",
       " 'pegfp',\n",
       " 'n1',\n",
       " 'EGFP',\n",
       " 'gene',\n",
       " 'utilize',\n",
       " 'reporter',\n",
       " 'gene',\n",
       " 'C6',\n",
       " 'Hep',\n",
       " 'G2',\n",
       " 'cell',\n",
       " 'line',\n",
       " 'optimal',\n",
       " 'n',\n",
       " 'p',\n",
       " 'ratio',\n",
       " 'polycation',\n",
       " 'evaluate',\n",
       " 'polymer',\n",
       " 'mediate',\n",
       " 'gene',\n",
       " 'transfection',\n",
       " 'transfected',\n",
       " 'cell',\n",
       " 'image',\n",
       " 'Leica',\n",
       " 'DMI3000B',\n",
       " 'fluorescence',\n",
       " 'microscope',\n",
       " 'percentage',\n",
       " 'EGFP',\n",
       " 'positive',\n",
       " 'cell',\n",
       " 'determine',\n",
       " 'flow',\n",
       " 'cytometry',\n",
       " 'Beckman',\n",
       " 'Coulter',\n",
       " 'Brea',\n",
       " 'USA']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokenizer(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the same code for the seed document\n",
    "Had to use the suggestion in https://stackoverflow.com/questions/42068474/tfidfvectorizer-how-does-the-vectorizer-with-fixed-vocab-deal-with-new-words to get the vocabulary from a single document.\n",
    "The attribute vocabulary_ is a dictionary with index and token.\n",
    "\n",
    "One has to use TfidfVectorizer with this tokenizer because numerical values get attached with each work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_run = TfidfVectorizer(input = 'content', tokenizer = spacy_tokenizer)\n",
    "tokensrun = token_run.fit_transform([abstracts[0]])\n",
    "len(token_run.vocabulary_)\n",
    "vocab = token_run.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'site': 43,\n",
       " 'preference': 37,\n",
       " 'ir': 22,\n",
       " 'temperature': 50,\n",
       " 'dependence': 10,\n",
       " 'magnetic': 28,\n",
       " 'susceptibility': 48,\n",
       " 'low': 27,\n",
       " 'specific': 44,\n",
       " 'heat': 18,\n",
       " 'β': 58,\n",
       " 'mn1−xirx': 30,\n",
       " 'alloy': 3,\n",
       " 'investigate': 21,\n",
       " 'phase': 34,\n",
       " 'exist': 15,\n",
       " 'x': 57,\n",
       " '=': 2,\n",
       " '0.102': 0,\n",
       " 'ray': 40,\n",
       " 'diffraction': 11,\n",
       " 'pattern': 33,\n",
       " 'atom': 5,\n",
       " 'substitute': 47,\n",
       " '1': 1,\n",
       " 'mn': 29,\n",
       " 'lattice': 24,\n",
       " 'electronic': 13,\n",
       " 'coefficient': 6,\n",
       " 'γ': 59,\n",
       " 'proportional': 38,\n",
       " 'neel': 31,\n",
       " 'tn': 51,\n",
       " 'form': 16,\n",
       " 'tn3/4': 52,\n",
       " 'region': 41,\n",
       " 'concentration': 7,\n",
       " 'increase': 19,\n",
       " 'gradual': 17,\n",
       " 'decrease': 9,\n",
       " 'lead': 25,\n",
       " 'relative': 42,\n",
       " 'straight': 46,\n",
       " 'line': 26,\n",
       " 'conclude': 8,\n",
       " 'state': 45,\n",
       " 'vary': 54,\n",
       " 'weak': 56,\n",
       " 'itinerant': 23,\n",
       " 'electron': 12,\n",
       " 'antiferromagnet': 4,\n",
       " 'intermediate': 20,\n",
       " 'point': 36,\n",
       " 'observe': 32,\n",
       " 'versus': 55,\n",
       " 'plot': 35,\n",
       " 'universal': 53,\n",
       " 'system': 49,\n",
       " 'entire': 14,\n",
       " 'range': 39}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above vocabulary to run the vectorizer on the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(input = 'content', vocabulary = vocab)\n",
    "result = tfidf_vector.fit_transform(corpus)\n",
    "#if (token_run.vocabulary_ == tfidf_vector.vocabulary_):\n",
    "#    print(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem with this method.\n",
    "There could be words in the other abstracts which have similar lemma's once processed by spacy but because the processing was not done for them the information is lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way for the system to work for me would be if I am able to do lazy loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "412 items in the list of tokens. A lot of useless words have seeped into this list. There has to be a better way to tokenize.\n",
    "New vocabulary leads to a change in the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1833, 79)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see if the system fails and then move to the next thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the size of the corpus is quite large, we want to analyze the documents one at a time, to pull out the term frequency per document and run the IDF later once we have complete information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer(input='content', vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = count_vector.fit_transform(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53364, 60) (53423, 60)\n"
     ]
    }
   ],
   "source": [
    "print(counts.shape,\n",
    "counts_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_1[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_1 = count_vector.fit_transform(abstracts_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = vstack([counts,counts_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full[53364].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d633a737026b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcount_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mabstracts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;31m# TfidfVectorizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1189\u001b[0m                 \u001b[0;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 \"string object received.\")\n",
      "\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer(input='content', vocabulary=vocab)\n",
    "for document in abstracts:\n",
    "    counts = count_vector.fit_transform(document)\n",
    "    print(list(document))\n",
    "    try:\n",
    "        matrix\n",
    "        matrix = vstack(matrix,counts)\n",
    "    except NameError:\n",
    "        matrix = vstack(counts)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = count_vector.fit_transform(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<53364x60 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 98723 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDFTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transform = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = transform.fit_transform(full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer.transform\n",
    "Has made me feel that fit_transform should be converted to transform.\n",
    "May have to be tested in some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106787"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "53364+53423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 3.72322259, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[9].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0069"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5**2+0.87**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = transform.fit_transform(full[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a possibility that this calculation only deals with single document instead of the sum.\n",
    "It might be that each vector is actually normalized. Simplifying the cosine similarity.\n",
    "Pulling the idf_ matrix out and looking at the number and calling the Transformer without normalization\n",
    "shows more reasonable values.\n",
    "It is normalization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.57860084, 12.57860084, 12.57860084,  5.95852763,  7.42530925,\n",
       "        5.25477027,  4.65452851,  4.72344383,  5.81010763,  4.86749959,\n",
       "        3.89758956,  4.63021555,  3.40554705,  4.52949312,  5.65595695,\n",
       "        5.29652718,  3.83188449,  7.25072467,  4.30833173,  4.047702  ,\n",
       "        5.23130114,  4.12200727,  5.6839308 ,  7.76641648,  4.38741484,\n",
       "        4.74302159,  4.05781411,  3.07730875,  3.25136672,  6.21385008,\n",
       "       12.57860084,  7.62277378,  5.34731384,  4.92605515,  3.172447  ,\n",
       "        7.18952911,  3.89521547,  7.91516175,  5.40158207,  3.16108371,\n",
       "        3.84114825,  3.72322259,  4.31346521,  5.53544092,  4.51964082,\n",
       "        3.26805321,  6.4452028 ,  8.35909313,  5.77843077,  2.93174899,\n",
       "        3.04032456,  8.05681226, 12.57860084,  5.73059557,  5.89649224,\n",
       "        5.98692711,  4.64265574, 12.57860084, 12.57860084, 12.57860084])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full[9].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cos_df = [ [ 0 for i in range(len(corpus)) ] for j in range(len(corpus)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x60 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    curr_cos_sim = linear_kernel(result[i:i+1], result).flatten()\n",
    "    cos_df[i] = curr_cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above matrix is for the reduced vocabulary and the lower one is for the complete vocabulary.\n",
    "As you can see there are the lower values have become even lower and the higher values have increased.\n",
    "Which makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.        , 0.35742576, 0.03255663, 0.04803749, 0.01320895,\n",
       "        0.16450696]),\n",
       " array([0.35742576, 1.        , 0.08864397, 0.01671493, 0.03596484,\n",
       "        0.02085738]),\n",
       " array([0.03255663, 0.08864397, 1.        , 0.18856254, 0.40572238,\n",
       "        0.23529381]),\n",
       " array([0.04803749, 0.01671493, 0.18856254, 1.        , 0.07650404,\n",
       "        0.0443676 ]),\n",
       " array([0.01320895, 0.03596484, 0.40572238, 0.07650404, 1.        ,\n",
       "        0.09546397]),\n",
       " array([0.16450696, 0.02085738, 0.23529381, 0.0443676 , 0.09546397,\n",
       "        1.        ])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix is not required for the complete calculation. I only need to generate the single vector and keep values below a certain number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_vector = linear_kernel(result[0],result).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106787"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cos_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94551733\n",
      "212848771\n",
      "12734688\n",
      "121299667\n",
      "111269941\n",
      "62824450\n",
      "1524750\n",
      "120516622\n",
      "120661898\n",
      "122011684\n",
      "3185812\n",
      "123800178\n",
      "120117731\n",
      "52115422\n",
      "120280905\n",
      "55046007\n",
      "124871465\n",
      "128298978\n",
      "124781275\n",
      "117072727\n",
      "39347270\n",
      "113990997\n",
      "121671340\n",
      "118972429\n",
      "123265753\n",
      "122929139\n",
      "118355429\n",
      "40649689\n",
      "123675267\n",
      "122861089\n",
      "15721065\n",
      "117431057\n",
      "117757255\n",
      "125027275\n",
      "121430800\n",
      "119914501\n",
      "121793427\n",
      "29507435\n",
      "25970692\n",
      "121400895\n",
      "122750063\n",
      "124869437\n",
      "119299890\n",
      "121247456\n",
      "100018553\n",
      "121472610\n",
      "120416053\n",
      "54895312\n",
      "203651849\n",
      "102899\n",
      "22093653\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cos_vector)):\n",
    "    if(cos_vector[i]>0.5):\n",
    "        print(paper_ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.27586522, 0.07364076, ..., 0.19852808, 0.        ,\n",
       "       0.07995279])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
